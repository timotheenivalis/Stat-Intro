\documentclass[10pt]{beamer}

\setbeamersize{text margin left=0.5cm, text margin right=0.5cm}

\usepackage{alltt}%
%\usetheme{Boadilla}
\usetheme[progressbar = foot, background=light]{metropolis} 
%\useoutertheme{split}

%\usepackage{listings}
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX


\usepackage[utf8]{inputenc}
\usepackage{default}

\usepackage{xcolor}%for color mixing

\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}

\usepackage{tikz}
\usepackage{multirow}
\usepackage{booktabs}

\setbeamertemplate{itemize/enumerate body begin}{\small}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Statistical Modelling: Beyond Linear Models, Generalized Linear Models}
\subtitle{Chapter 5}
\author{Timoth\'ee Bonnet}
\date{\today}

\begin{document}

%\lstset{language=R}%code

\setbeamerfont{section in toc}{size*={14}{16}}
\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{}
    \tableofcontents[currentsection,sectionstyle=show/shaded,subsectionstyle=show/shaded/hide]% down vote\tableofcontents[currentsection,currentsubsection,hideothersubsections,sectionstyle=show/hide,subsectionstyle=show/shaded/hide] 
  \end{frame}
}



\begin{frame}[plain]{Where to find help?}
    \begin{block}{}
        \begin{itemize}
         \item The Internet
         \item Colleagues
         \item (books: learn BEFORE you have an issue)
         \item Courses, workshops, consulting
        \end{itemize}
  \pause  
    \includegraphics[width=\textwidth]{Figures/bdsi}
    BDSI: Terry Neeman, Marcin Adamski, Cameron Jack, myself \dots
    \end{block}
\pause
    \alert{\large + R-Ladies, Coding Club}
\end{frame}
%%%%%%%%%%%%


\begin{frame}{}
\maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Simple linear models}
  \textbf{{\color{purple}{Response}} = {\color{blue}{Intercept}} + {\color{red}{Slope}} $\times$ {\color{orange}{Predictor}} + {\color{gray}{Error}}} \\

\centering
\includegraphics[width=0.6\textwidth]{Figures/figure/lmprinc-1}
\end{frame}
%%%%%%%%%%%%


\begin{frame}{Linear model basic assumptions}
 \begin{block}{}
     \begin{itemize}
      \item Predictor not perfectly correlated \\ \textit{Risk: Model won't run, unstable convergence, or huge SE}
       \item {\color{red!20!black}{Little error in predictors}}\\ \textit{Risk: bias estimates (underestimate with Gaussian error)}
       \item {\color{red!50!black}{Gaussian error distribution}}\\ \textit{Risk: Poor predictions}
       \item {\color{red!70!black}{Homoscedasticity (constant error variance)}}\\ \textit{Risk: Over-optimistic uncertainty, unreliable predictions}
       \item {\color{red!99!black}{Independence of error}}\\ \textit{Risk: Bias and over-optimistic uncertainty}
     \end{itemize}
 \end{block}
\end{frame}
%%%%%%%%%%%%


\begin{frame}{A simple linear model failure: binary data}

\centering
\only<1>{\includegraphics[width=0.6\textwidth]{Figures/figure/binlmprinc-1}}
\only<2>{\includegraphics[width=0.6\textwidth]{Figures/figure/binlmprinc3-1}

  \begin{alertblock}{Assumptions violated:}
    Non-Gaussian errors, non-constant error variance, correlated errors
  \end{alertblock}
  }


\end{frame}
%%%%%%%%%%%%%

\begin{frame}{What we want our model to do}
\centering
\includegraphics[width=0.5\textwidth]{Figures/logreg1-1.pdf}

\begin{block}{What we need:}
  \begin{enumerate}[<+->]
    \item Convert the predictor open scale ($-\infty$ to $+\infty$) to a bounded scale (0 to 1)
    \item Acknowledge discrete data
    \item Response variability depends on expected value
  \end{enumerate}
\end{block}
\end{frame}
%%%%%%%%%%%

\begin{frame}{That is what a Generalized Linear Model does}

\begin{block}{Vocabulary warning}
  \begin{itemize}
    \item General Linear Model (=linear model with several responses, multivariate)
    \item \textbf{Generalized Linear Model (=non-normal errors, and uncertainty dependent on the mean)} 
  \end{itemize}
\end{block}

\pause

\begin{block}{What a GLM is:}
  \begin{enumerate}[<+->]
    \item \textbf{Linear function} (reponse = intercept + slope $\times$ predictor \dots)
    \item ``\textbf{Link function}" = a map between the linear function ($-\infty$ to $+\infty$) and a probability distribution (from 0 to 1 for Bernouilli)
    \item \textbf{Probability distribution} (Bernouilli, Binomial, Poisson\dots) thought to generate the data (either 0 or 1 for Bernouilli)
  \end{enumerate}
\pause[\thebeamerpauses]

GLMs fit continuous expected response; we observe discrete realizations
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%


\section{Binary data}

\begin{frame}{Logistic regression}

  \begin{block}{}
   
\begin{itemize}[<+->]
    \item Binary or proportion data (survival, presence/absence\dots)
    \item Binomial probability distribution ( = Bernouilly if binary data)
    \item Link function often logit: $y=\log(\frac{probability}{1-probability})$
    \item Linear function $y = intercept + slope_1 predictor_1 + slope_2 predictor_2 +$
  \end{itemize}
  
  
  \end{block}

  \end{frame}
  %%%%%%%%%%%%
  
\begin{frame}[fragile]{What is the Bernouilli distribution?}

    
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
bernouilli_random_sample <- rbinom(n = 10000, size = 1, prob = 0.3)
hist(bernouilli_random_sample)
mean(bernouilli_random_sample); 0.3
var(bernouilli_random_sample); 0.3*(1-0.3)
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}
%%%%%%%%%%%%


\begin{frame}{What to do with logistic regression}
\centering
\only<1-2>{\includegraphics[width=0.5\textwidth]{Figures/logreg1-1.pdf}}

\only<3>{
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.843, 0.867, 0.922}\color{fgcolor}
\includegraphics[width=0.5\textwidth,height=0.5\textheight]{Figures/logreg2-1} 

\end{knitrout}
}

\only<4>{
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.843, 0.867, 0.922}\color{fgcolor}
\includegraphics[width=0.5\textwidth,height=0.5\textheight]{Figures/logreg3-1} 

\end{knitrout}
}

\begin{block}{}
\begin{enumerate}[<+->]
  \item Response increase/decrease with increasing predictor?
  \item Estimate probability of 0/1 given a predictor value
  \item Predict 0/1 and classify predictor values ($\rightarrow$ Machine Learning)
\end{enumerate}
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%
  
\begin{frame}[fragile]{Logistic regression in R} 

    
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
glm(formula = obs ~ 1 + x, family = "binomial", data=data)
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}
%%%%%%%%%%%%
\begin{frame}{Does survival probability depend on size?}
 
 \begin{exampleblock}{Exercise, part 1}
    \begin{enumerate}
      \item Load \texttt{survivalsize.csv}
      \item Plot survival data. What kind of distribution is it?
      \item Logistic GLM of survival as a function of size. How does size correlates with survival?
      \item What is the unit of coefficients?
    \end{enumerate}
  \end{exampleblock}
  
\end{frame}
%%%%%%%%%%%%


\begin{frame}{Back-transformation}
\begin{center}
\begin{tikzpicture}
\node (sc) at (-6, 0.5) {\textbf{Scales:}};
\uncover<2->{
  \node (mininf) at (-4,0) {$- \infty $};
  \node (maxinf) at (4,0) {$+ \infty $};
  \node (binf) at (-3,0) {};
  \node (tinf) at (3,0) {};
  \node (zer) at (0,0) {$0$};
  \draw (binf) -- (zer) -- (tinf);
  \draw[dashed] (binf)--(mininf);
  \draw[dashed] (tinf)--(maxinf);
  \node (modest) at (-6,0) {Model estimates};
}
\uncover<3->{
  \node (prb) at (-6,-2) {Probabilities};
  \node (minpr) at (-4,-2) {0};
  \node (maxpr) at (4,-2) {1};
  \node (zerp) at (0,-2) {$0.5$};
  \draw (minpr)-- (zerp) -- (maxpr);
}
\uncover<4->{
  \node (dat) at (-6,-4) {Data};
  \node (mind) at (-4,-4) {\large \textbf{0}};
  \node (maxd) at (4,-4) {\large \textbf{1}};
  \draw[dashed] (mind)--(maxd);
}

\uncover<5->{

\draw[->, draw=red] (zer) -- (zerp);
\draw[->, draw=red] (mininf) -- (minpr);
\draw[->, draw=red] (maxinf) -- (maxpr);

\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (-0.5,0) -- (-0.7,-2);
\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (0.5,0) -- (0.7,-2);

\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (-1,0) -- (-2,-2);
\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (1,0) -- (2,-2);


\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (-1.5,0) -- (-3.8,-2);
\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (1.5,0) -- (3.8,-2);

}

\uncover<6->{
\draw[->, draw=red, dashed, line width=2pt] (zerp) -- (mind);
\draw[->, draw=red, dashed, line width=2pt] (zerp) -- (maxd);

\draw[->, draw=red, dashed, line width=4pt] (minpr) -- (mind);
\draw[->, draw=red, dashed, line width=4pt] (maxpr) -- (maxd);

\draw[->, draw=red, dashed, line width=3pt] (-3,-2) -- (mind);
\draw[->, draw=red, dashed, line width=0.5pt] (-3,-2) -- (maxd);

\draw[->, draw=red, dashed, line width=3pt] (3,-2) -- (maxd);
\draw[->, draw=red, dashed, line width=0.5pt] (3,-2) -- (mind);
}
\end{tikzpicture}
\end{center}

\uncover<7->{
\begin{block}{Conversion:}
\begin{itemize}
  \item from model to probability: $p=\frac{1}{1+\exp(-x)}$ or \texttt{plogis(x)}
  \item probability and data on same scale, but continuous/discrete
  \item $\exp(slope)$ = odd-ratio
\end{itemize}
\end{block}
}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Does survival probability depend on size?}
 
  \begin{exampleblock}{Exercise, part 2}
    \begin{enumerate}
      \item Load \texttt{survivalsize.csv}
      \item Fit a linear model and a logistic model with intercept only. How to interpret the estimates?
    \end{enumerate}
  \end{exampleblock}
  
  \pause
  
  \begin{block}{hints:}
  \begin{enumerate}
   \item For a given predicted $y$, $\exp(y)$ is the odd ratio: probability success / probability failure
   \item Back-transformation inverse-logit: $probability = \frac{1}{1 + exp(-y)}$
  \end{enumerate}
  \end{block}

  
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Solutions part 2}
     
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\small
\begin{verbatim}
surv <- read.csv("Data/survival.csv")
plot(surv$survival)
lmsurv <- glm(survival~1, data=surv, family=gaussian)
lregsurv <- glm(survival~1, data=surv, family=binomial)

#linear model prediction:
coefficients(lmsurv)

#logistic reg prediction:
plogis(coefficients(lregsurv))
1/(1+exp(-coefficients(lregsurv)))
exp(coefficients(lregsurv))

#observed mean survival:
mean(surv$survival)
#mean odd-ratio:
mean(surv$survival)/(1-mean(surv$survival))
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}
%%%%%%%%%%%


\begin{frame}{Does survival probability depend on size?}
 
  \begin{exampleblock}{Exercise, part 3}
    \begin{enumerate}
      \item Fit a linear regression and a logistic regression of survival on relative size, compare the outputs
      \item Check the diagnostic plots for both models. Should you be worried?
      \item Extract and visualize a model prediction from both models (use the function predict(), and/or do it by hand to practice link-function back-transformation)
    \end{enumerate}
  \end{exampleblock}
  
\end{frame}
%%%%%%%%%%%


\begin{frame}[fragile]{Solutions part 3}
     
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\footnotesize
\begin{verbatim}
lmsurvS <- glm(survival~1 + relative_size, data=surv, family=gaussian)
lregsurvS <- glm(survival~1 + relative_size, data=surv, family=binomial)

summary(lmsurvS)
summary(lregsurvS)

plot(lmsurvS)
plot(lregsurvS)

plot(surv$relative_size, surv$survival, ylim=c(-0.2,1.2))
abline(lmsurv, col="red")

plot(surv$relative_size, surv$survival, ylim=c(-0.2,1.2))
datforpred <- data.frame(relative_size=seq(from=-3,to=4, by=0.1))
datforpred$prob <- predict(lregsurvS, newdata = datforpred,
type = "response")
lines(datforpred$relative_size, datforpred$prob, col="red")

ggplot(surv, aes(x = relative_size, y=survival))+geom_point()+
stat_smooth(method = "glm", method.args = list(family = "binomial"))

\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}
%%%%%%%%%%%

\begin{frame}{Model assumptions}
\begin{block}{Logistic regression assumes:}
\begin{itemize}[<+->]
  \item \textbf{Binary data}
  \item No unaccounted source of correlations in the date (e.g., pseudo-replication, spatial autocorrelations, phylogenetic signal\dots)
  \item (no error in the predictors)
  \item (no complete separation = only 0s or only 1s for some predictor level)
\end{itemize}
\end{block}

\pause

NO assumptions about the distribution of residuals (Normality, homoscedasticity).\\
BUT more assumptions in non-binary GLMs (proportions and count data)!!

\end{frame}
%%%%%%%%%%%

\begin{frame}{More practice: does survival probability depend on weight? does the relationship depend on sex?}
 
  \begin{exampleblock}{Exercise}
    \begin{enumerate}
      \item Load \texttt{survivalweight.csv}
      \item Plot data
      \item Fit a logistic model to address these questions
      \item Plot the results
    \end{enumerate}
  \end{exampleblock}
 \end{frame}
%%%%%%%%%%%%%


\section{Count data}

\begin{frame}{Poisson regression}
  \begin{itemize}
    \item Count data
    \item Poisson distribution
    \item Link function: logarithm
    \item Inverse link function: exponential
    \item Linear function $y = intercept + slope_1 predictor_1 + slope_2 predictor_2 +$ \dots
  \end{itemize}
\end{frame}
%%%%%%%%%%%%


\begin{frame}[fragile]{What is the Poisson distribution?}

    
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
poisson_random_sample <- rpois(n = 10000, lambda = 4)  
hist(poisson_random_sample)
mean(poisson_random_sample)
var(poisson_random_sample)
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}
%%%%%%%%%%%%

  
\begin{frame}[fragile]{Poisson regression in R}
    
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}

glm(formula = obs ~1 + x, family = "poisson", data=data)

glm(formula = obs ~1 + x, family = "quasipoisson", data=data)
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{alertblock}{family = ``poisson" is dangerous}
 \begin{itemize}
  \item A true Poisson distribution has $E(\exp(Y))=V(\exp(Y))$
  \item Assumes no unexplained variation in $Y$
  \item \texttt{glm()} follows this assumption
  \item In nature,  $E(\exp(Y)) < V(\exp(Y))$ most of the time
  \item SE and p-value to small
  \item family = "quasipoisson" correct the uncertainty in \texttt{glm()}
  \item or mixed model with \texttt{(1|obs)}
  \item other packages never follow the assumption (\texttt{MCMCglmm})
 \end{itemize}
\end{alertblock}
%%%%%%%%%%%%%%%%

\end{frame}
%%%%%%%%%%%%

\begin{frame}{Practice with Poisson glm}
 
  \begin{exampleblock}{Exercise}
    \begin{enumerate}
      \item Load the data reproduction.csv
      \item Plot reproduction data, calculate the mean and variance. 
      \item Overlay a Gaussian distribution of same mean and variance, does it fit?
      \item Fit an compare a lm and a Poisson glm of reproduction on size 
      \item Check the diagnostic plots for both models. Should you be worried?
      \item Extract and visualize a model prediction from both models (use the function predict, and/or do it by hand to practice link-function back-transformation)
      \item Before GLMs, researchers used to log-transform the data and fit linear models. What are the problems with this approach?
    \end{enumerate}
  \end{exampleblock}
\end{frame}
%%%%%%%%%%%%

\begin{frame}{Can we decrease aggressive behavior in noisy miners?}
\begin{columns}
 \begin{column}{0.6\textwidth}
 \begin{block}{Context}
  \begin{itemize}
    \item ``Harassment.Data.csv''
    \item Outcome measure: number of attacks\\
    \item Experimental factor: Removal of noisy miners (Control/Treatment); Just-After Treatment / long-term (``Phase'')\\
    \item Data: 6 farms, 8 one-hour surveys for each combination
    \end{itemize}
   \end{block}
  \end{column}
  \begin{column}{0.4\textwidth}
   \includegraphics[width=0.9\textwidth]{Figures/noisyminer}
  \end{column}

\end{columns}

 
\end{frame}
%%%%%%%%%%%%


\end{document}
